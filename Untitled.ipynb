{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "                   [--show_ground_truth SHOW_GROUND_TRUTH]\n",
      "                   [--model_name MODEL_NAME] [--half_size HALF_SIZE]\n",
      "__main__.py: error: unrecognized arguments: -f /run/user/1000/jupyter/kernel-3267d246-1e1b-4ff2-9167-a7fc44380921.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import h5py\n",
    "import argparse\n",
    "from scipy.ndimage.measurements import center_of_mass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_POINTS = 68\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Test trained SDN models')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=20, help='Evaluation batch size')\n",
    "parser.add_argument('--show_ground_truth', type=bool, default=False, help='Show ground truth or not')\n",
    "parser.add_argument('--model_name', help='The name of model you want to test')\n",
    "parser.add_argument('--half_size', type=bool, default=True, help='If the model is trained with half_sized label')\n",
    "\n",
    "opt = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_landmarks(image,prediction):\n",
    "    image=image.astype(np.uint8)\n",
    "    image=np.transpose(image,(1,2,0))\n",
    "    image = cv2.resize(image,(224, 224))\n",
    "    # if ground_truth.shape==(136,):\n",
    "    #     ground_truth=np.reshape(ground_truth,(68,2))\n",
    "\n",
    "\n",
    "    for i in range(0, prediction.shape[0]):\n",
    "        cv2.circle(image, center=(int(prediction[i][1]), int(prediction[i][0])), radius=1, color=(0,255,0), thickness=-1)\n",
    "    # if opt.show_ground_truth:\n",
    "    #     for i in range(0, ground_truth.shape[0]):\n",
    "    #        cv2.circle(image, center=(int(ground_truth[i][1]), int(ground_truth[i][0])), radius=1, color=(0,0,255), thickness=-1)\n",
    "    return image\n",
    "\n",
    "def get_normalized_inter_ocular_distance(predictions,ground_truths):\n",
    "    \"\"\"\n",
    "    Get the distance normalized by inter-ocular distance\n",
    "\n",
    "    Args:\n",
    "    1. Predictions from the network shape: (N,N_POINTS,2)\n",
    "    2. Ground truth  shape: (N,N_POINTS,2)\n",
    "\n",
    "    Return:\n",
    "    The normalized inter-ocular distance\n",
    "    \"\"\"\n",
    "    distances = np.linalg.norm(predictions-ground_truths, axis= -1) # shape: (N,N_POINTS)\n",
    "    inter_ocular_distances = np.linalg.norm(ground_truths[:,37,:]-ground_truths[:,46,:], axis= -1,keepdims=True) # shape: (N,1)\n",
    "    return (np.mean(distances/inter_ocular_distances))\n",
    "\n",
    "\n",
    "def get_predictions_float(np_heatmaps,threshold=0.95):\n",
    "    \"\"\"\n",
    "    Get the point coordinates prediction result in float from the heatmaps\n",
    "\n",
    "    Args:\n",
    "        network_output: tensor, float32 - [N,C,H,W]\n",
    "    \n",
    "    Returns:\n",
    "        result: tensor, float32 - [N,C,2]\n",
    "    \"\"\"\n",
    "    N = np_heatmaps.shape[0]\n",
    "    C = np_heatmaps.shape[1]\n",
    "    predictions = np.zeros((N,C,2))\n",
    "\n",
    "    for n_batch in range(N):\n",
    "        for n_channel in range(C):\n",
    "            max_value = np.amax(np_heatmaps[n_batch,n_channel])\n",
    "            low_value_indices=np_heatmaps[n_batch,n_channel]<(threshold*max_value)\n",
    "            np_heatmaps[n_batch,n_channel][low_value_indices]=0\n",
    "            predictions[n_batch,n_channel]=np.array(center_of_mass(np_heatmaps[n_batch,n_channel]))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def eval_in_batches(data,sess):\n",
    "# Get all test_loss for a dataset by running it in small batches.\"\"\"\n",
    "# Small utility function to evaluate a dataset by feeding batches of data to\n",
    "# {test_images_node} and pulling the results from {eval_predictions}.\n",
    "# Saves memory and enables this to run on smaller GPUs.\n",
    "\n",
    "    size = data.shape[0]\n",
    "    if size < opt.eval_batch_size:\n",
    "      raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "\n",
    "    eval_pred = np.ndarray(shape=(size, N_POINTS,2), dtype=np.float32)\n",
    "\n",
    "    if opt.half_size:\n",
    "        lbl_size = 112\n",
    "    else:\n",
    "        lbl_size = 224\n",
    "\n",
    "    eval_heatmap = np.ndarray(shape=(size, N_POINTS,lbl_size,lbl_size), dtype=np.float32)\n",
    "\n",
    "    print('Testing...')\n",
    "    for begin in range(0, size, opt.eval_batch_size):\n",
    "        end = begin + opt.eval_batch_size\n",
    "        if end <= size:\n",
    "            network_output,network_output_heatmap = sess.run([test_prediction,test_heat_map],\n",
    "            feed_dict={test_images_node: data[begin:end, ...], mode: False})\n",
    "\n",
    "            # eval_pred[begin:end, :] = np.asarray(network_output)\n",
    "\n",
    "            eval_pred[begin:end, :] = get_predictions_float(network_output_heatmap)\n",
    "\n",
    "            # eval_heatmap[begin:end, :] = np.asarray(network_output_heatmap)\n",
    "        else:\n",
    "            network_output, network_output_heatmap = sess.run([test_prediction,test_heat_map],\n",
    "            feed_dict={test_images_node: data[-opt.eval_batch_size:, ...],mode: False})\n",
    "            # eval_pred[begin:, :] = np.asarray(network_output[begin - size:, :])\n",
    "\n",
    "            eval_pred[begin:, :] = get_predictions_float(network_output_heatmap[begin - size:, :])\n",
    "\n",
    "            # eval_heatmap[begin:, :] = np.asarray(network_output_heatmap[begin - size:, :])\n",
    "\n",
    "    return  eval_pred,eval_heatmap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "###############Loading Test Data########################\n",
    "########################################################\n",
    "\n",
    "h5_file = h5py.File(\"/home/yongzhe/Project/make_dataset/heatmap/int112/test.h5\",\"r\")\n",
    "test_set_image = h5_file['image_set']\n",
    "test_set_label_heatmap = h5_file['label_set']\n",
    "\n",
    "cor_h5_file = h5py.File(\"/home/yongzhe/Project/make_dataset/coordiantes/tensorflow/test.h5\",\"r\")\n",
    "test_set_label = cor_h5_file['label_set']\n",
    "\n",
    "# test_set_image = h5py.File(\"./data/new_helen_train_image.h5\",\"r\")['dataset']\n",
    "# test_set_label = h5py.File(\"./data/new_helen_train_label.h5\",\"r\")['dataset']\n",
    "\n",
    "\n",
    "#########################################################\n",
    "#########################################################\n",
    "model_path = './model/'+opt.model_name+'.ckpt'\n",
    "saver = tf.train.import_meta_graph(model_path+'.meta')\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, model_path)\n",
    "    # all_vars = tf.trainable_variables()\n",
    "    test_heat_map = tf.get_collection('network_output')[0]\n",
    "    test_prediction = tf.get_collection('predictions')[0]\n",
    "    test_images_node = tf.get_collection('images_node')[0]\n",
    "    mode= tf.get_collection('mode')[0]\n",
    "    prediction, _ = eval_in_batches(test_set_image,sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
